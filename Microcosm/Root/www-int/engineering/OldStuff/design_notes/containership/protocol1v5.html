<HTML>
<HEAD>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=windows-1252">
<META NAME="Generator" CONTENT="Microsoft Word 97">
<TITLE>FIRST NOTATION:</TITLE>
</HEAD>
<BODY>

<FONT SIZE=2><P>FIRST PROTOCOL VERSION 5:</P>
<P>&nbsp;</P>
<P>Change history:</P>

<UL>
<LI>protocol1v3: integrated Paul’s comments, fleshed out uSetRoot description for Brian</LI>
<LI>protocol1v4: touched up state diagram for Paul, added location information properly throughout, added description of composition protocol at end, fixed Brian’s errata</LI>
<LI>protocol1v5: fixed up entries at start (thanks Bill), added &quot;throw if Cbintransit&quot; in step 8 (thanks Trev/Arturo), added several other fixes from Brian</LI></UL>

<P>&nbsp;</P>
<P>BRIEF INTRODUCTORY NOTE</P>
<P>&nbsp;</P>
<P>This is a complicated protocol.  Here’s why:</P>
<P>&nbsp;</P>
<P>The basic containment problem is as follows.  There is a region which contains a set of objects.  The region is an unum hosted on some machine R.  There are some other machines X, Y, Z all connected to machine R, all with client presences of the region unum.</P>
<P>&nbsp;</P>
<P>When an object (let’s say an avatar hosted on machine A) wants to enter the region, it sends a request to the host presence of the region unum on machine R, containing information about what the avatar wants to look like.  The host presence verifies that that appearance is suitable under the TOS of the region.  If it is, the region tells machines X, Y, and Z that the avatar has entered the region, and spreads client presences of the avatar to machines X, Y, and Z.  In order for the avatar to come to be in the shared space of the region, it must gain admittance via the region’s host presence.</P>
<P>&nbsp;</P>
<P>Now consider the case where the avatar, hosted on machine A, is carrying a tray, hosted on machine T, on which is an obscene object, hosted on machine O.  The avatar and the tray are both &quot;open containers&quot;, which is to say that they and all their contents are visible in whatever region they enter.  Now when the avatar seeks to enter the region hosted on R, it needs to present information not just describing itself but also describing the appearance of T and of O.  And the region needs to decide not only whether A passes TOS but also whether R and O do.  Only if <U>all</U> pass the TOS check should A be admitted.</P>
<P>&nbsp;</P>
<P>(An alternative possible protocol would be for A to enter the region R, then to tell T to come along; T could then attempt to enter, and if it succeeded, it could ask O to join it; and so on, avoiding the need for A to present information upfront about both T <U>and</U> O.  The problem here is that if O fails to pass TOS, it’s too late for A and T; they are already in the new region.  A winds up in R holding an empty tray.  What happened to O?   And what if the user decides it was a mistake to try to enter R in the first place?  You can’t always get back where you came from once you leave a region….)</P>
<P>&nbsp;</P>
<P>The protocol below deals with this problem by storing in each separate container a copy of the entire state of the open containment tree below that container.  For example, A contains a tree with the entries {A, T, O}; T contains a tree with {T, O}; etc.  This means that whenever a container seeks to enter another container, it can present all the information up front.   We decided to think of A’s transition as a transaction wherein A attempts to enter on the basis of its current state regarding itself and all its contents.  The complexity of the protocol below is largely concerned with keeping consistent state between containers, their contents, and their parents.</P>
<P>&nbsp;</P>
<P>PREFACE</P>
<P>&nbsp;</P>
<P>This document walks through the containment protocol for the following configuration of objects.  Note that some messages (uFlush, uParentAddUnum, uRemove) are only sent to a container’s parent if the container is not the root.  Since any container may or may not be the root, there are cases in this protocol writeup where the message flow would be different if, for example, OC was actually a root—i.e. was actually OR.  These cases are not called out explicitly; it is the case that OC and NC show the behavior of non-root containers when receiving these messages, and OR and NR show the behavior of root containers when receiving these messages.</P>
<P>&nbsp;</P>
<P>Also, in any instance where a container sends a message to its parent, it is the case that if there are additional parents between that container and the root, the message will be handled identically by each such parent; for example, uFlush, uParentAddUnum, and uRemove requests all propagate all the way up the parent chain however long it is. </P>
<P>&nbsp;</P>
<P>Soon this protocol will need review to ensure that all message sends are wrapped by catches that properly handle disconnection or revocation failures.  This has not yet been begun, but the basic story is:  if a container receives a disconnection/revocation exception from a containable, it does the equivalent of &quot;OCfsupCb <FONT FACE="Wingdings">&#223;</FONT>
 uRemove()&quot;; if a containable gets disconnection/revocation from a container, it transfers itself to its container of last resort (which perforce must be hosted on its agency or the object’s not guaranteed to be able to reach it); and that may be all the mechanism we need.  We’ll see.</P>
<P>&nbsp;</P>
<P>CAST OF OBJECTS</P>
<P>&nbsp;</P>

<UL>
<LI>OR: old root<BR>
ORkeyOC: OR’s key for OC as a direct child (i.e. OR’s &quot;direct-containment&quot; key for OC) <BR>
ORfsupOC: OR’s facet supporting OC as direct child (contains ORkeyOC)ORrootkeyOC: OR’s key for OC as a child under this root (i.e. OR’s &quot;indirect-open-containment&quot; key for OC)<BR>
ORrootkeyCb: OR’s key for Cb as a child under this rootORfsupOC: OR’s facet supporting OC as parent (contains ORkeyOC as state) <BR>
ORfrootsupOC: OR’s facet supporting OC as root (contains ORrootkeyOC as state) <BR>
ORfrootsupCb: OR’s facet supporting OC as root (contains ORrootkeyCb as state) <BR>
(and likewise ORrootkeyCbchild, ORfrootsupCbchild)<BR>
ORchildtree: OR’s tree of children</LI>
<LI>OC: old container<BR>
OCparent: OC’s channel to its parent (initially ORfsupOC) <BR>
OCroot: OC’s channel to its root (initially ORfrootsupOC) <BR>
OCunumrouter: OC’s unum router<BR>
OCfsupOR: OC’s facet supporting OR (i.e. CbfsupOC is to OC as OCfsupOR is to OR)<BR>
OCkeyCb: OC’s key for Cb<BR>
OCfsupCb: OC’s facet supporting Cb (contains OCkeyCb)<BR>
OCpendingrequests: OC’s table of pending flush requests from below<BR>
OCdirectchildren: OC’s vector of *fsupOC facets for communicating with its children (initially just CbfsupOC)<BR>
OCchildtree: OC’s tree of children</LI></UL>


<UL>
<LI>Cb: containable <BR>
Cbparent: Cb’s channel to its parent (initially OCfsupCb)<BR>
Cbroot: Cb’s channel to its root’s facet (initially ORfrootsupCb) <BR>
Cbunumrouter: Cb’s unum router<BR>
CbfsupOC: Cb’s facet supporting OC<BR>
Cbchildtree: Cb’s tree of children<BR>
Cbfrozenchildtree: Cb’s tree of children, cloned at uTrySetContainer time<BR>
Cbintransit: boolean flag, true if in transit<BR>
Cbfnewtransfer: facet made by containable when it receives a uRequestTransfer message<BR>
Cbfnewsetparent: facet made by containable when it receives a uTrySetContainer message</LI></UL>


<UL>
<LI>NC: new container<BR>
(NCfsetparent, NCunumrouter, as for Cb)<BR>
NCparent: NC’s channel to its parent (initially NRfsupNC)<BR>
NCroot: NC’s channel to its root (initially NRfrootsupNC) <BR>
NCfadd: public add facet for NC (note that this is potentially sturdyable!) <BR>
NCdirectchildren: NC’s vector of *fsupNC facets for communicating with its children<BR>
NCchildtree: NC’s tree of children<BR>
NCkeyCb: NC’s new key for Cb<BR>
NCfsupCb: NC’s facet supporting Cb (contains NCkeyCb)</LI>
<LI>NR: new root<BR>
NRkey: new root’s key<BR>
NRkeyNC: NR’s key for NC<BR>
NRfsupNC: NR’s facet supporting NC as parent (contains NRkeyNC)<BR>
NRrootkeyCb: NR’s key for CbNRfrootsupCb: NR’s facet supporting Cb as root (contains NRrootkeyCb) <BR>
(and likewise NRrootkeyCbchild, NRfrootsupCbchild)<BR>
NRchildtree: NR’s tree of children</LI></UL>

<P>&nbsp;</P>
<P>Messages to objects:</P>

<UL>
<LI>Root receives messages: </LI>
<LI>uAddUnum (add an entire branch under a given container contained directly by this root)</LI>
<LI>uParentAddUnum (add an entire branch under a given container which is a (possibly nested) child of some direct child of this root)</LI>
<LI>uRemove (remove an entire branch directly under this root)</LI>
<LI>uParentRemove (remove an entire branch under some nested child of this root)</LI>
<LI>uFlush (if all changes flushed, return a flushAck)</LI>
<LI>Container receives messages: </LI>
<LI>uRequestContainableTransfer (request to transfer a containable out of this container)</LI>
<LI>uAddUnum (add a containable into this container)</LI>
<LI>uParentAddUnum (if this container has containers as children, one of the children just got an unum added to it)</LI>
<LI>uParentAddFailed (some add to one of our nested-children failed and we need to clean up our tree and let them know)</LI>
<LI>uRemove (remove a containable directly under this container)</LI>
<LI>uParentRemove (remove a containable under some nested child of this container)</LI>
<LI>uFlush (verify all changes processed)</LI>
<LI>uFlushAck (propagate an acknowledgement on downwards)</LI>
<LI>Containable receives messages: </LI>
<LI>uRequestTransfer (request to transfer this containable to another container)</LI>
<LI>uTrySetContainer (commence an add of this containable elsewhere)</LI>
<LI>uAddFailed (add attempt failed)</LI>
<LI>uFlushAck (all parents of this container have processed all messages from this containable prior to the flush)</LI>
<LI>uSetParent (add has committed, set the parent of this containable) </LI>
<LI>uSetRoot (this containable or a container above it has moved to a new root; recreate unumrouter, etc.)</LI></UL>

<P>&nbsp;</P>
<P>Notation for protocol flow:</P>
<P>X on I(a,b):</P>
<P>action</P>
<P>action</P>
<P>Y <FONT FACE="Wingdings">&#223;</FONT>
 J(a,b)</P>
<P>means that on receipt of message I(a,b), X performs two actions and then sends message J(a,b) to Y.</P>
<P>&nbsp;</P>
<P>The protocol initiates as follows:</P>
<P>&nbsp;</P>
<OL>

<LI>initiator:<BR>
Cb <FONT FACE="Wingdings">&#223;</FONT>
 uRequestTransfer(NCfadd, toNotifyFail, location) <BR>
// toNotifyFail is an object to inform if the transfer fails</LI>
<LI>Cb on uRequestTransfer(NCfadd, toNotifyFail, location): <BR>
if !Cbintransit {<BR>
    make new facet Cbfnewtransfer with CbfsupOC, NCfadd, toNotifyFail<BR>
    Cbparent <FONT FACE="Wingdings">&#223;</FONT>
 uRequestContainableTransfer(Cbfnewtransfer, location) <BR>
} else {<BR>
    toNotifyFail <FONT FACE="Wingdings">&#223;</FONT>
 transferFailed(&quot;containable already in transit&quot;) <BR>
}<BR>
<I>Commentary: We do not have to reject the transfer request here; we could just let it queue in Cbparent if we are currently in transit.  Arturo would prefer for now that we reject upfront.  We can change this later if upfront rejection turns out painful.</LI>
</I><LI>OCfsupCb on uRequestContainableTransfer(Cbfnewtransfer, location): <BR>
 OC <FONT FACE="Wingdings">&#223;</FONT>
 uRequestContainableTransfer(OCkeyCb, Cbfnewtransfer, location)</LI>
<LI>OC on uRequestContainableTransfer(OCkeyCb, Cbfnewtransfer, location): <BR>
validate that transfer is OK<BR>
if so {<BR>
    Cbfnewtransfer <FONT FACE="Wingdings">&#223;</FONT>
 uTrySetContainer(OCkeyCb, location) <BR>
} else {<BR>
    Cbfnewtransfer <FONT FACE="Wingdings">&#223;</FONT>
 uTransferFailed(&quot;because container wouldn’t let it go&quot;)<BR>
}</LI>
<LI>Cbfnewtransfer on uTransferFailed(reasonWhy): <BR>
toNotifyFail <FONT FACE="Wingdings">&#223;</FONT>
 uTransferFailed(reasonWhy) <BR>
revoke Cbfnewtransfer <I>// no reason a facet can’t revoke itself….</LI>
</I><LI>toNotifyFail on uTransferFailed(reasonWhy): <BR>
Notify user of failure and reasonWhy.</LI>
<LI>Cbfnewtransfer on uTrySetContainer(OCkeyCb, location): <BR>
CbfsupOC <FONT FACE="Wingdings">&#223;</FONT>
 uTrySetContainer(NCfadd, toNotifyFail, OCkeyCb, location) <BR>
revoke myself</LI>
<LI>CbfsupOC on uTrySetContainer(NCfadd, toNotifyFail, OCkeyCb, location):<BR>
if CbfsupOCisqueueing { <I>// see step 15; this is how explicit queueing is implemented</I><BR>
    CbfsupOCqueue <FONT FACE="Wingdings">&#223;</FONT>
 uTrySetContainer(NCfadd, toNotifyFail) <BR>
} else {<BR>
    Cb <FONT FACE="Wingdings">&#223;</FONT>
 uTrySetContainer(NCfadd, toNotifyFail) <BR>
}</LI>
<LI>Cb on uTrySetContainer(NCfadd, toNotifyFail, OCkeyCb, location):<BR>
<I>// NOTE that this is the only place where we do &quot;optimistic channeling&quot;.  Here is where we will<BR>
// channel-deadlock, if anywhere. <BR>
</I><BR>
Cbintransit = truecreate new channel Cbnewparent, and its associated distributor &amp;Cbnewparent <BR>
create new channel Cbqueueparent, and &amp;Cbqueueparent <I>// see commentary below</I><BR>
create new channel Cbqueueroot, and &amp;Cbqueueroot<BR>
Cboldparent = Cbparent<BR>
Cboldroot = Cbroot<BR>
create new Cbfrozenchildtree as a clone of Cbchildtree <I>// if Cb has no kids, Cbchildtree has one entry for Cb</I><BR>
 if Cb is a container and has any children {<BR>
<BR>
    create new random key Cbflushkey, and store it in CbfsupOC (see step 16)<BR>
    Cbparent <FONT FACE="Wingdings">&#223;</FONT>
 uFlush(Cbflushkey) <I>// goes to OCfsupCb since Cbparent is initially forwarded to OCfsupCb</I> <BR>
} else {<BR>
    go synchronously to step#16, skipping the whole flush protocol<BR>
    <I>// we can go synchronously to CbfsupOC since CbfsupOC is colocated with Cb</I><BR>
<I>    // i.e. do the equivalent of CbfsupOC<FONT FACE="Wingdings">&#223;</FONT>
uFlushAck(…)</I><BR>
}<BR>
Cbparent = Cbqueueparent<BR>
Cbroot = Cbqueueroot<BR>
<BR>
<I>Commentary: As of now, Cb is in transit as far as its children are concerned.  Cb is syncing with its upwards parents.  Note that until the uFlushAck is received, Cb may still be subject to messages from above, so we do not put CbfsupOC into a queueing state until the uFlushAck completes. </I><BR>
<I>Also note that Cbqueueparent is different from Cbnewparent!  Cbqueueparent is the channel into which messages from below go until we commit or abort.  Cbnewparent is the channel that gets hooked up to NCfsupCb as soon as NC processes uAddUnum.  &amp;Cbqueueparent<FONT FACE="Wingdings">&#223;</FONT>
forward(Cbnewparent) will happen if we commit; &amp;Cbqueueparent <FONT FACE="Wingdings">&#223;</FONT>
 forward(Cboldparent) will happen if we abort (step 20).</I> <BR>
<I>Cbfrozenchildtree is an addition of Arturo’s after he noticed the following race.  When we get u{Parent}{Add,Remove}Unum messages from our children, we immediately update Cbchildtree and then relay the messages to Cbparent.  If we are in transit (i.e. Cbintransit is true) and waiting for an uFlushAck (i.e. we are between this step and step#17),  and we receive such a message from a kid, then we will be updating Cbchildtree in a way that our parents don’t know about yet.  Then if we pass that updated childtree off to uAddUnum we will be propagating uncommitted state.</I> <BR>
<I>The fix is either to queue all changes to Cbchildtree (not just all messages to Cbparent), or to make a snapshot of Cbchildtree and send <U>that</U> to uAddUnum when the time comes.  We take the latter strategy, creating Cbfrozenchildtree, and updating it only with uParentAddFailed messages from above.</LI>
</I><LI>OCfsupCb on uFlush(Cbflushkey): <BR>
OC <FONT FACE="Wingdings">&#223;</FONT>
 uFlush(OCkeyCb, Cbflushkey)</LI>
<LI>OC on uFlush(OCkeyCb, Cbflushkey): <BR>
create entry in OCpendingrequests for [Cbflushkey, OCkeyCb] <BR>
<I>// the above entry represents &quot;when we get uFlushAck(Cbflushkey), forward it to OCkeyCb (i.e. Cb)&quot;</I><BR>
OCparent <FONT FACE="Wingdings">&#223;</FONT>
 uFlush(Cbflushkey) <BR>
<I>// note that if OC is in transit, this uFlush will get queued in the unforwarded OCparent channel</LI>
</I><LI>ORfsupOC on uFlush(Cbflushkey): <BR>
OR <FONT FACE="Wingdings">&#223;</FONT>
 uFlush(ORkeyOC, Cbflushkey)</LI>
<LI>OR on uFlush(ORkeyOC, Cbflushkey)<BR>
<I>// if we receive uFlush at the root, we just ack immediately</I><BR>
look up OCfsupOR using ORkeyOC<BR>
OCfsupOR <FONT FACE="Wingdings">&#223;</FONT>
 uFlushAck(Cbflushkey)</LI>
<LI>OCfsupOR on uFlushAck(Cbflushkey): <BR>
OC <FONT FACE="Wingdings">&#223;</FONT>
 uFlushAck(Cbflushkey)<BR>
<I>// pretty boring? yes… but the point is that OCfsupOR is revokable when/if OC leaves OR</LI>
</I><LI>OC on uFlushAck(Cbflushkey): <BR>
look up and remove Cbflushkey in OCpendingrequests<BR>
look up OCkeyCb in OCchildtree, to obtain CbfsupOC<BR>
CbfsupOC <FONT FACE="Wingdings">&#223;</FONT>
 uFlushAck(Cbflushkey) <BR>
<BR>
<I>Commentary: As of now, OC has &quot;given permission&quot; for Cb to leave, but as far as OC is concerned Cb is still its child.  However, in the next step, CbfsupOC goes into a queueing state until either Cb commits (in which case all such messages get dropped—we need a way to make this throw exceptions appropriately), or until Cb aborts, in which case those queued messages will get forwarded.</LI>
</I><LI>CbfsupOC on uFlushAck(Cbflushkey): <BR>
if Cbflushkey is the one we remember {<BR>
    set flag CbfsupOCisqueueing = true<BR>
    create channel CbfsupOCqueue &amp; associated distributor &amp;CbfsupOCqueue<BR>
    Cb <FONT FACE="Wingdings">&#223;</FONT>
 uFlushAck(Cbflushkey) <BR>
} else {<BR>
    Cb <FONT FACE="Wingdings">&#223;</FONT>
 uFlushAck(Cbflushkey) <I>// to forward on down</I> <BR>
}<BR>
<BR>
<I>Commentary: At this stage, Cb is in transit as far as OC is concerned.  OC doesn’t need to <U>know</U> this, though, since now that CbfsupOC is queueing messages from OC, OC can act as though Cb is still present—just with longer latency.</LI>
</I><LI>Cb on uFlushAck(Cbflushkey): <BR>
verify that Cbflushkey is the key we made<BR>
<I>// commence tryadd protocol, now that we know that Cbchildtree is synced as of our flush</I><BR>
create new facet Cbfnewsetparent, containing toNotifyFail (for committing) <BR>
create new facet CbfsupNC, initially in queueing mode<BR>
NCfadd <FONT FACE="Wingdings">&#223;</FONT>
 uAddUnum(Cbfrozenchildtree, Cbfnewsetparent, CbfsupNC, &amp;Cbnewparent, OCkeyCb, location) <BR>
<BR>
<I>Commentary: As of this point, we have created a facet to support our new container, but we don’t yet know whether we are <U>in</U> our new container.  So we set that facet to queue messages to it until we know.  This is how we avoid having to lock the new container; we just let it optimistically communicate to us, and if we later fail to get added to some parent of it, we just drop/throw those messages.</LI>
</I><LI>NCfadd on uAddUnum(Cbfrozenchildtree, Cbfnewsetparent, CbfsupNC, &amp;Cbnewparent, OCkeyCb, location): <BR>
NC <FONT FACE="Wingdings">&#223;</FONT>
 uAddUnum(Cbfrozenchildtree, Cbfnewsetparent, CbfsupNC, &amp;Cbnewparent)</LI>
<LI>NC on uAddUnum(Cbfrozenchildtree, Cbfnewsetparent, CbfsupNC, &amp;Cbnewparent, OCkeyCb, location):<BR>
verify that Cbfrozenchildtree passes NC’s TOS <I>// should be doable synchronously with only Cbchildtree as state</I><BR>
if it does {<BR>
    create new key NCkeyCb<BR>
    insert Cbfrozenchildtree into the root of NCchildtree at NCkeyCb<BR>
    create new facet NCfsupCb containing NCkeyCb<BR>
<I>    // wire up the facets<BR>
</I>    &amp;Cbnewparent <FONT FACE="Wingdings">&#223;</FONT>
 forward(NCfsupCb)<BR>
    store CbfsupNC in NCdirectchildren at NCkeyCb<BR>
    NCparent <FONT FACE="Wingdings">&#223;</FONT>
 uAddUnum(Cbfrozenchildtree, Cbfnewsetparent, OCkeyCb, NCkeyCb, location)<BR>
} else {<BR>
    construct explanation whyAddFailed<BR>
    Cbfnewsetparent <FONT FACE="Wingdings">&#223;</FONT>
 uAddFailed(whyAddFailed)<BR>
}<BR>
<BR>
<I>Commentary:  As of now, Cb is a child of NC.  NC may attempt to send it containment messages via CbfsupNC. Cb won’t process these messages until CbfsupNC gets changed out of queueing mode, which happens below in uSetParent. <BR>
Note that right here is one motivation for Cbfrozenchildtree.  Consider what would happen if NC were in transit here.  If NCintransit, then NCchildtree as of right now is being updated with children that may not eventually make it in.  We do not want to consider those kids—i.e. Cb et al.—part of our state when we go to call uAddUnum on NC’s parent-to-be, since we haven’t gotten confirmation (via uFlushAck) that the Cbfrozenchildtree kids have made it in to NC.  So we would pass NCfrozenchildtree to uAddUnum.  But this is not the scenario we’re evaluating here….</LI>
</I><LI>Cbfnewsetparent on uAddFailed(whyAddFailed): <I>// explain the failure cases inline…</I><BR>
Cb <FONT FACE="Wingdings">&#223;</FONT>
 uAddFailed(whyAddFailed, toNotifyFail)</LI>
<LI>Cb on uAddFailed(whyAddFailed, toNotifyFail):<BR>
&amp;Cbqueueparent <FONT FACE="Wingdings">&#223;</FONT>
 forward(Cboldparent) <I>// send all queued messages to old parent/root</I><BR>
&amp;Cbqueueroot <FONT FACE="Wingdings">&#223;</FONT>
 forward(Cboldroot) <BR>
Cbintransit = false<BR>
discard Cbfrozenchildtree<BR>
revoke Cbfnewsetparent and CbfsupNC<BR>
discard Cbnewparent, Cbnewroot <BR>
do the following synchronously on CbfsupOC: {<BR>
    CbfsupOCisqueueing = false<BR>
<I>    // this is how messages are unqueued</I><BR>
    &amp;CbfsupOCqueue <FONT FACE="Wingdings">&#223;</FONT>
 forward(CbfsupOC) <BR>
}<BR>
toNotifyFail <FONT FACE="Wingdings">&#223;</FONT>
 uTransferFailed(whyAddFailed) <BR>
<BR>
<I>Commentary: As of now, Cb is back under OC as if nothing had ever happened.</I> <BR>
<I>Further Commentary:  The logic here is also what should happen if Cb times out while waiting to receive a uSetParent call.  The idea is that this step is what’s involved in aborting the transaction with respect to Cb, whether because of timeout or because the new container or some parent rejected the add.  Brian points out that you may obtain speedier cleanup if you do &quot;Cbnewparent <FONT FACE="Wingdings">&#223;</FONT>
 uRemove()&quot; before discarding Cbnewparent in this timeout case.  This &quot;abort-the-Cb-transit&quot; timeout seems to handle all the malicious-behavior and adding-a-containable-under-itself deadlocks we have found.  Prove us wrong, please!  <FONT FACE="Wingdings">&#74;</FONT>
</LI>
</I><LI>NRfsupNC on uParentAddUnum(Cbfrozenchildtree, Cbfnewsetparent, OCkeyCb, NCkeyCb, location): <BR>
NR <FONT FACE="Wingdings">&#223;</FONT>
 uParentAddUnum(NRkeyNC, Cbfrozenchildtree, Cbfnewsetparent, OCkeyCb, NCkeyCb, location)</LI>
<LI>NR on uParentAddUnum(NRkeyNC, Cbfrozenchildtree, Cbfnewsetparent, OCkeyCb, NCkeyCb, location): <BR>
if Cbfrozenchildtree’s root key (i.e. OCkeyCb) is already in NRchildtree {<BR>
    <I>// NR == OR, so we just move Cbchildtree under this root; no need to check TOS since it’s already here</I><BR>
    move Cbfrozenchildtree to be under NC at NCkeyCb in NRchildtree<BR>
    <I>// … later when we get the remove, OCkeyCb will not be present in NR’s tree anymore</I><BR>
    <I>// this is where we can optimize by using the old presenters &amp; knowing not to revoke the old frootsup</I><BR>
<I>    // facets; see note at end of step#30</I>    <BR>
<I>    //<BR>
    // following code is what the root does; if NR was instead an intermediate parent, it would just now do<BR>
    // NRparent <FONT FACE="Wingdings">&#223;</FONT>
 uParentAddUnum(Cbfrozenchildtree, Cbfnewsetparent, OCkeyCb, NCkeyCb, location).<BR>
</I>    <I>// but back to the root code: <BR>
</I>    Cbfnewsetparent <FONT FACE="Wingdings">&#223;</FONT>
 uSetParent(null, null) <BR>
    <I>// no args needed since Cb and its childrencan use their current root facets</I><BR>
    &lt;all presences of NR&gt; <FONT FACE="Wingdings">&#223;</FONT>
 pMoveBranch(OCkeyCb, NCkeyCb, location) <I>// see composition writeup</I><BR>
    return<BR>
}<BR>
<I>// OK, it’s not the same root, so do the whole setparent logic</I><BR>
verify Cbfrozenchildtree passes NR’s TOS <I>// should be doable synchronously with only Cbchildtree as state</I><BR>
if it does {<BR>
    add Cbfrozenchildtree to NR’s entry for NC at NCkeyCb in NRchildtree<BR>
    create new empty trees NRchanneltree and NRdistributortree<BR>
    for each entry Cbchild in Cbfrozenchildtree (<U>including Cb’s entry</U>): {<BR>
        create a new channel Cbchildchannel and distributor &amp;Cbchildchannel<BR>
        create a new key NRrootkeyCbchild<BR>
        create a new root facet NRfrootsupCbchild containing NRrootkeyCbchild<BR>
        put new entry [Cbchildpstate, Cbchildchannel] into NRchanneltree<BR>
        put new entry [&amp;Cbchildchannel, NRfrootsupCbchild] into NRdistributortree<BR>
    }<BR>
    Cbfnewsetparent <FONT FACE="Wingdings">&#223;</FONT>
 uSetParent(NRfsupCb, NRdistributortree) <BR>
    &lt;all clients of NR&gt; <FONT FACE="Wingdings">&#223;</FONT>
 pAddBranch(NRchanneltree) <I>// fan out NRchanneltree; see composition writeup</I><BR>
} else {<BR>
    <I>// doesn’t pass TOS, propagate the removes back down</I><BR>
    construct explanation whyAddFailed<BR>
    look up NCfsupNR using NRkeyNC<BR>
    NCfsupNR <FONT FACE="Wingdings">&#223;</FONT>
 uParentAddFailed(NCkeyCb, Cbfsetparent, whyAddFailed)<BR>
}</LI>
<LI>NCfsupNR on uParentAddFailed(NCkeyCb, Cbfsetparent, whyAddFailed): <I>// again, cover the exception first</I><BR>
NC <FONT FACE="Wingdings">&#223;</FONT>
 uParentAddFailed(NCkeyCb, Cbfsetparent, whyAddFailed)</LI>
<LI>NC on uParentAddFailed(NCkeyCb, Cbfsetparent, whyAddFailed):<BR>
look up NCkeyCb in NCchildtree, yielding Cbfrozenchildtree<BR>
remove Cbfrozenchildtree from NCchildtree<BR>
if we have NCfrozenchildtree (i.e. if NCintransit), remove Cbfrozenchildtree from that, too<BR>
Cbfnewsetparent <FONT FACE="Wingdings">&#223;</FONT>
 uAddFailed(whyAddFailed) <I>// already described above</LI>
</I><LI>Cbfnewsetparent on uSetParent (NRfsupCb, NRdistributortree):<BR>
<I>// OK, connect all our plumbing!</I> <BR>
discard Cbfrozenchildtree<BR>
Cbintransit = false<BR>
Cboldparent <FONT FACE="Wingdings">&#223;</FONT>
 uRemove() <I>// Cboldparent is still OCfsupCb</I><BR>
&amp;Cbqueueparent <FONT FACE="Wingdings">&#223;</FONT>
 forward(Cbnewparent) <I>// wire us to our new parent</I><BR>
revoke CbfsupOC <I>// and drop any messages inside it</I><BR>
if (NRfsupCb == null) {<BR>
    <I>// we are under the same root, so send our root-queued messages to old root</I><BR>
    &amp;Cbqueueroot <FONT FACE="Wingdings">&#223;</FONT>
 forward(Cboldroot) <BR>
} else {<BR>
    <I>// we are NOT under the same root!</I> <BR>
    get NRfrootsupCb out of base entry of NRdistributortree<BR>
    <I>//</I> <I>since Cb’s entry in NRdistributortree is</I><BR>
<I>    // the root entry of the tree, and since we need to forward our Cbqueueroot messages to NRfrootsupCb</I><BR>
    &amp;Cbqueueroot <FONT FACE="Wingdings">&#223;</FONT>
 forward(NRfrootsupCb)<BR>
    Cb.uSetRoot(NRdistributortree) <I>// synchronous call, otherwise there’s a race window here</I><BR>
}<BR>
<BR>
<I>Commentary: As of now, we are all wired up to our parent.  If we moved roots, though, we still have to start propagating the root-changed notification downwards, via our *fsupCb facets.  Note that if any of our children are in transit, the root-change message will queue in their facet; and if they go somewhere else, the root-change message will bounce, which is Just Fine.</I> <BR>
<I>Note that we can discard Cbfrozenchildtree since its job is done.  Actually we could have discarded it as soon as we passed it to uAddUnum…..</LI>
</I><LI>OCfsupCb on uRemove():<BR>
OC <FONT FACE="Wingdings">&#223;</FONT>
 uRemove(OCkeyCb)</LI>
<LI>OC on uRemove(OCkeyCb): <BR>
remove OCkeyCb’s entry from OCdirectchildren and OCchildtree<BR>
revoke OCfsupCb<BR>
ORfsupOC <FONT FACE="Wingdings">&#223;</FONT>
 uParentRemove(OCkeyCb)</LI>
<LI>ORfsupOC on uParentRemove(OCkeyCb): <BR>
OR <FONT FACE="Wingdings">&#223;</FONT>
 uParentRemove(ORkeyOC, OCkeyCb)</LI>
<LI>OR on uParentRemove(ORkeyOC, OCkeyCb):<BR>
remove entry for Cb from ORchildtree<BR>
revoke ORfsupCb<BR>
<BR>
<I>Commentary: It is assumed here that OCkeyCb is the key for Cb’s branch in ORchildtree, not just in OCchildtree.  That is, OCkeyCb is a meaningful key for everyone with a tree containing Cb’s branch.  This has no security implications since just possessing OCkeyCb is not enough to spoof a request-from-Cb to OC or anyone else.  Spoofing OCkeyCb for purposes of &quot;is this the same branch&quot; is still a <B>SEMI-OPEN ISSUE</B>.  ANOTHER semi-open issue is when do you revoke the root facets ORfrootsupCb and ORfrootsupCbchild?  You want to revoke them only when you have not readded the children elsewhere in the root.  This is not critical for correctness though… the likely story will be to check whether the children are <U>already</U> elsewhere in the root and if so don’t revoke, otherwise, do.  This will work because we always causalitywise add before we remove.</LI>
</I><LI>Cb on uSetRoot (NRdistributortree): <BR>
revoke Cbunumrouter/presence router/client vector<BR>
create Cbnewunumrouter/etc/etc<BR>
Cbroot = NRfrootsupCb <I>// root entry of NRdistributortree contains &amp;Cbchannel and NRfrootsupCb</I><BR>
<I>// note that we already did this for Cb itself, but since we recurse on uSetRoot, we will need to reset the root</I><BR>
<I>// pointers of all of Cb’s children on down, so we notate it this way here</I> <BR>
&amp;Cbchannel <FONT FACE="Wingdings">&#223;</FONT>
 forward(Cbnewunumrouter) <BR>
for each of Cbchild’s direct children {<BR>
    get key CbkeyCbchild<BR>
    get Cbchildbranch from NRdistributortree<BR>
    Cbchild <FONT FACE="Wingdings">&#223;</FONT>
 uSetRoot(Cbchildbranch) <BR>
}<BR>
<BR>
<I>Commentary:  The idea here is that we continue propagating uSetRoots downwards towards our children, passing each one the sub-branch of distributors that it and its kids need to forward.  Hopefully I don’t need to spell out that Cbchild will do the same thing as Cb; it will set its Cbchildroot to NRfrootsupCbchild, it will forward &amp;Cbchildchannel to the new unum router it makes for itself, etc.</LI></OL>

</I><P>&nbsp;</P>
<P>COMPOSITION WRITEUP</P>
<P>(hopefully verbose enough to minimize the need for a meeting on this <FONT FACE="Wingdings">&#74;</FONT>
)</P>
<P>The composition protocol adds relatively little to the containership protocol.  There are two fundamental aspects to composition:</P>

<UL>
<LI>securely granting una access to presenters they can use to display themselves, despite the fact that presenters are local objects which cannot be sent across the network</LI>
<LI>allowing una that remain under one root to continue using their presenters as they move from container to container within that root (since presenters are moderately expensive to create, involving loading art, etc., etc.)</LI>
<LI>ensuring that updates in presenter location and relative parenting are absolutely consistent across machines</LI></UL>

<P>The composition ingredients essentially piggyback on the containership ingredients, maintaining consistency and security as follows:</P>
<OL>

<LI>Roots (host and clients) create presenters when they are informed of a new (i.e. not previously under that root) branch’s arrival.  They hand these presenters down to the new una via the channels in NRchanneltree.  Thus a given presence of an unum on machine X can only get its hands on a presenter by being handed one from its root presence on machine X.  (Also, a root presence on machine X itself has to get the capabilities to create presenters on that machine; this is done when an avatar, i.e. a user, specifically chooses to make that region the current rootcompositor on that machine.  See the uLocalSetRootCompositor protocol below.)</LI>
<LI>Roots maintain the location information and hierarchy information for all presenters handed out by the root.  Only the root retains the capability to make a presenter relocate itself visually (whether that means just changing location, or changing its actual presenter parent).  The presenter capabilities handed down by the root do not allow una to perform change-location or set-parent operations; they must request the root to perform these operations.  This guarantees that all machines will see a consistent view of the presenters in the region, as far as location and hierarchy are concerned.</LI>
<LI>When a containable moves from one container to another under the same root, the root recognizes, at the top of the containership protocol, that this has happened; the root can rearrange the presenter hierarchy suitably without needing to recreate all presenters from scratch.</LI>
<LI>Roots hand down root-supporting facets (i.e. NRfrootsupCb et al.) which containers can use to make requests to move their children.  Containables cannot use their root facet to request that they be relocated; only their container can request the root to relocate them.  This is because containers are the rightful arbiter of where a containable is within them, so the root will only accept &quot;please move this containable within me&quot; messages from a container’s frootsup facet, rather than &quot;please move me within my container&quot; messages from a containable’s frootsup facet.</LI></OL>

<P>[DO NOT BE AFRAID!  This is broadly similar to what the current composition protocol does; this general structure is known to work.]</P>
<P>There is one more key point: location.  The location of a containable within a container is a bit of state which sits on the borderline of containership itself.  Originally we split containership from composition thinking that containership had only to do with <U>whether</U> something is contained, and composition was a wholly separate protocol on top dealing with <U>where</U> that something is.</P>
<P>This turns out not to be right given the need to support containers which can reject add requests because the location is not available; i.e. the current containership protocol needs to have location worked into it at the NC uAddUnum level, so uAddUnum can react properly if the location is invalid.</P>
<P>So the current story is that:</P>
<OL>

<LI>Containables request to be added to a container at a specific location (the location parameter to uAddUnum).</LI>
<LI>That location gets stored in Cb’s entry in NC, NCP, etc. all the way up to NR.</LI>
<LI>When Cb wants to change its location in NC, it sends a uRequestMove message to NC.</LI>
<LI>NC can do it or not.  If NC agrees, NC then sends a uRequestContainableMove message to NR via NCroot (i.e. NRfrootsupNC).</LI>
<LI>NR then updates its record of where Cb is located within NC, and fans that out to all NR’s presences, all of which move Cb’s presenter appropriately.</LI></OL>

<P>What this implies is that the location state of any intermediate parents (i.e. NCP here) does not get updated after the add; the uRequestContainableMove message does not propagate via the parent chain (i.e. via NCparent) but goes straight to the root (via NCroot, i.e. NRfrootsupNC).  This means that the location state of intermediate parents will get stale, but since only the root is responsible for the locations visible onscreen, this doesn’t matter… except when a branch appears under a new root.  Then all containers in the branch must tell their new root where their kids are currently located.  This is done via the uPlaceKids message sent via the root facets.</P>
<P>To sum up, here are the additional composition messages:</P>
<P>&nbsp;</P>

<UL>
<LI>Messages sent to the root (i.e. NR) via root facets (NRfsupCb et al.):</LI>
<LI>uRequestContainableMove (NCkeyCb, newLocation) // move my kid underneath me</LI>
<LI>uPlaceKids (Hashtable kidLocations)  // update the stale location state for my kids</LI></UL>


<UL>
<LI>Messages sent to a local presence of the root (i.e. NRclient) from a host avatar:</LI>
<LI>uLocalSetRootCompositor (PresentationCapabilities) // make presenters for yourself &amp; everything within you using these capabilities I’m granting you; i.e. it’s ultimately the user who allows a region to display itself on the user’s machine; these capabilities are only usable on this machine</LI>
<LI>uLocalKillRootCompositor () // go away, I don’t want to see you any more</LI></UL>


<UL>
<LI>Messages sent to all presences of NR from NR itself:</LI>
<LI>pAddBranch (NRchanneltree) // this is how fanout is actually done</LI>
<LI>pMovePresenter (NCkeyCb, location) // this is how roots fan out move requests</LI>
<LI>pMoveBranch (OCkeyCb, NCkeyCb, location) // move an entire branch  </LI>
<LI>pPlaceKids (NRrootkeyCb, Hashtable kids) // update the locations of all the kids of this entry</LI>
<LI>Messages sent to a container (i.e. NC) by a compositable (Cb):</LI>
<LI>uRequestMove (newLocation) // request relocation within the container</LI>
<LI>Messages sent to a containable (i.e. Cb) by a root (i.e. NR):</LI>
<LI>uLocalSetPresenter (presenter) // here’s your presenter; you can’t move it or set-parent it</LI></UL>

<P>And here is the additional composition state:</P>
<P>&nbsp;</P>

<UL>
<LI>Root presences have a presenter entry in each field in NRchildtree, and a presenter field for their root presenter</LI>
<LI>Containables have a presenter field for their presenter</LI></UL>

<P>So the protocol description here is fairly light on details, especially of the data structures.  To see some code look at compositable_kind.plu for the definitions of PresenterTree (in fact, Brian, you might want to look here in general as this is quite similar to what Cbchildtree et al. will look like).</P>
<P>CONTAINABLE MOVE PROTOCOL:</P>
<OL>

<LI>initiator sends<BR>
Cb <FONT FACE="Wingdings">&#223;</FONT>
 uRequestMove(newlocation)</LI>
<LI>Cb on uRequestMove(newlocation):<BR>
Cbparent <FONT FACE="Wingdings">&#223;</FONT>
 uRequestMove(newlocation)</LI>
<LI>NCfsupCb on uRequestMove(newlocation): <BR>
NC <FONT FACE="Wingdings">&#223;</FONT>
 uRequestMove(NCkeyCb, newlocation)</LI>
<LI>NC on uRequestMove(NCkeyCb, newlocation):<BR>
determine whether Cb can validly go to newLocation<BR>
if so: {<BR>
    NCroot <FONT FACE="Wingdings">&#223;</FONT>
 uRequestContainableMove(NCkeyCb, newlocation) <BR>
} else {<BR>
    ethrow CantMoveThereException(reasonwhy) <BR>
}</LI>
<LI>NRfrootsupNC on uRequestContainableMove(NCkeyCb, newlocation): <BR>
NR <FONT FACE="Wingdings">&#223;</FONT>
 uRequestContainableMove(NRrootkeyNC, NCkeyCb, newlocation)</LI>
<LI>NR on uRequestContainableMove(NRrootkeyNC, NCkeyCb, newlocation): <BR>
verify that NCkeyCb is the key of a child of NC’s<BR>
&lt;all presences of NR&gt; <FONT FACE="Wingdings">&#223;</FONT>
 pMovePresenter(NCkeyCb, newlocation)</LI>
<LI>&lt;all presences of NR&gt; on pMovePresenter(NCkeyCb, newlocation): <BR>
move the presenter whose key is NCkeyCb to newlocation (see current composition code for details)</LI></OL>

<P>CONTAINABLE ARRIVAL PROTOCOL (picks up from step 23 where pAddBranch is sent):</P>
<OL>

<LI>&lt;all presences of NR&gt; on pAddBranch(NRchanneltree): <BR>
if this presence is currently the rootcompositor on this machine, for each entry in NRchanneltree {<BR>
    make presenter Cbpresenter from Cbpstate<BR>
    Cbchannel <FONT FACE="Wingdings">&#223;</FONT>
 uLocalSetPresenter(Cbpresenter) <BR>
}</LI>
<LI>&lt;local presence of Cb forwarded to Cbchannel&gt; on uLocalSetPresenter (Cbpresenter): <BR>
set up Cbpresenter as our presenter</LI>
<LI>MEANWHILE, from step 31 above, all containers in NRchanneltree are sending uPlaceKids messages up to NR, so using Cb as an example: <BR>
NRfrootsupCb on uPlaceKids(Hashtable kidLocations): <BR>
NR <FONT FACE="Wingdings">&#223;</FONT>
 uPlaceKids(NRrootkeyCb, kidLocations)</LI>
<LI>NR on uPlaceKids(NRrootkeyCb, kidLocations): <BR>
&lt;all presences of NR&gt; <FONT FACE="Wingdings">&#223;</FONT>
 pPlaceKids(NRrootkeyCb, kidLocations)</LI>
<LI>&lt;all presences of NR&gt; on pPlaceKids(NRrootkeyCb, kidLocations): <BR>
look up NRrootkeyCb in NRchildtree<BR>
if this presence is currently the rootcompositor on this machine, update all the presenters that are children of Cb</LI></OL>

<P>CONTAINABLE RELOCATE-UNDER-SAME-ROOT PROTOCOL:</P>
<OL>

<LI>&lt;all presences of NR&gt; on pMoveBranch(OCkeyCb, NCkeyCb, newlocation): <BR>
move presenter branch at OCkeyCb to be under NCkeyCb at newlocation</LI>
<P>COMPOSITOR SET-ROOT PROTOCOL:</P>
<LI>&lt;local presence of NR&gt; <FONT FACE="Wingdings">&#223;</FONT>
 uLocalSetRootCompositor (PresenterCapabilities): <BR>
for each entry Cb in NRchildtree {<BR>
    make presenter Cbpresenter from Cbpstate<BR>
    save Cbpresenter in Cb’s entry in NRchildtree<BR>
    Cbchannel <FONT FACE="Wingdings">&#223;</FONT>
 uLocalSetPresenter(Cbpresenter) <BR>
}</LI>
<LI>&lt;local presence of NR&gt; <FONT FACE="Wingdings">&#223;</FONT>
 uLocalKillRootCompositor ():<BR>
for each entry Cb in NRchanneltree {<BR>
    revoke presenter Cbpresenter<BR>
    remove Cbpresenter from Cb’s entry<BR>
}</LI></OL>

<P>AND THAT’S ALL, FOLKS!</P>
<P>I believe this is consistent as none of these messages can affect the structure of the tree, just the location information, and all location information that the user can see is centralized via the host root.  The worst that can happen is if a root presence makes an entire presenter tree using the stale information it was passed (i.e. NCP transfers under NR2, bringing along stale location information for NC); this will get updated very quickly when NC’s uPlaceKids message gets fanned out to the overeager root presence.</P>
<P>The other problem with all this is it necessitates two network messages (containable-&gt;container, container-&gt;root) to make something start moving onscreen.  We cannot do better if we really wish to have containment location mediated by the container but presentation consistency centralized at the root.  However, we can optimize by having the local presence of the container forward a uLocalHintRequestContainableMove message to the local presence of the root; this allows the user’s machine to react immediately, lowering percieved latency for user actions.  The current composition code does this quite effectively.</P>
<P>&nbsp;</P></FONT></BODY>
</HTML>
